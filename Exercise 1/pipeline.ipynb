{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages and define helper functions and objects\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import NearMiss\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, roc_auc_score  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Converts time string to float value\n",
    "def string_to_timestamp(date_string):\n",
    "    time_stamp = time.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "    return time.mktime(time_stamp)\n",
    "\n",
    "#Helper objects for encoding the categorical data\n",
    "(issuercountry_set, txvariantcode_set, currencycode_set, shoppercountry_set, interaction_set,\n",
    "verification_set, accountcode_set, mail_id_set, ip_id_set, card_id_set) = [set() for _ in range(10)]\n",
    "(issuercountry_dict, txvariantcode_dict, currencycode_dict, shoppercountry_dict, interaction_dict,\n",
    "verification_dict, accountcode_dict, mail_id_dict, ip_id_dict, card_id_dict) = [{} for _ in range(10)]\n",
    "sm = SMOTE(random_state=42)\n",
    "nm = NearMiss()\n",
    "cc= ClusterCentroids(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads the data from the given csv\n",
    "def get_raw_data() : \n",
    "    ah = open('data_for_student_case.csv', 'r')\n",
    "    data = []\n",
    "    ah.readline()#skip first line\n",
    "    for line_ah in ah:\n",
    "        if line_ah.strip().split(',')[9]=='Refused':# remove the row with 'refused' label, since it's uncertain about fraud\n",
    "            continue\n",
    "        if 'na' in str(line_ah.strip().split(',')[14]).lower() or 'na' in str(line_ah.strip().split(',')[4].lower()):\n",
    "            continue\n",
    "        bookingdate = string_to_timestamp(line_ah.strip().split(',')[1])# date reported flaud\n",
    "        issuercountry = line_ah.strip().split(',')[2]#country code\n",
    "        issuercountry_set.add(issuercountry)\n",
    "        txvariantcode = line_ah.strip().split(',')[3]#type of card: visa/master\n",
    "        txvariantcode_set.add(txvariantcode)\n",
    "        issuer_id = float(line_ah.strip().split(',')[4])#bin card issuer identifier\n",
    "        amount = float(line_ah.strip().split(',')[5])#transaction amount in minor units\n",
    "        currencycode = line_ah.strip().split(',')[6]\n",
    "        currencycode_set.add(currencycode)\n",
    "        shoppercountry = line_ah.strip().split(',')[7]#country code\n",
    "        shoppercountry_set.add(shoppercountry)\n",
    "        interaction = line_ah.strip().split(',')[8]#online transaction or subscription\n",
    "        interaction_set.add(interaction)\n",
    "        if line_ah.strip().split(',')[9] == 'Chargeback':\n",
    "            label = 1#label fraud\n",
    "        else:\n",
    "            label = 0#label save\n",
    "        verification = line_ah.strip().split(',')[10]#shopper provide CVC code or not\n",
    "        verification_set.add(verification)\n",
    "        cvcresponse = int(line_ah.strip().split(',')[11])#0 = Unknown, 1=Match, 2=No Match, 3-6=Not checked\n",
    "        if cvcresponse > 2:\n",
    "            cvcresponse = 3\n",
    "        year_info = datetime.datetime.strptime(line_ah.strip().split(',')[12],'%Y-%m-%d %H:%M:%S').year\n",
    "        month_info = datetime.datetime.strptime(line_ah.strip().split(',')[12],'%Y-%m-%d %H:%M:%S').month\n",
    "        day_info = datetime.datetime.strptime(line_ah.strip().split(',')[12],'%Y-%m-%d %H:%M:%S').day\n",
    "        creationdate = str(year_info)+'-'+str(month_info)+'-'+str(day_info)#Date of transaction \n",
    "        creationdate_stamp = string_to_timestamp(line_ah.strip().split(',')[12])#Date of transaction-time stamp\n",
    "        accountcode = line_ah.strip().split(',')[13]#merchantâ€™s webshop\n",
    "        accountcode_set.add(accountcode)\n",
    "        mail_id = int(float(line_ah.strip().split(',')[14].replace('email','')))#mail\n",
    "        mail_id_set.add(mail_id)\n",
    "        ip_id = int(float(line_ah.strip().split(',')[15].replace('ip','')))#ip\n",
    "        ip_id_set.add(ip_id)\n",
    "        card_id = int(float(line_ah.strip().split(',')[16].replace('card','')))#card\n",
    "        card_id_set.add(card_id)\n",
    "        data.append([issuercountry, txvariantcode, issuer_id, amount, currencycode,\n",
    "                    shoppercountry, interaction, verification, cvcresponse, creationdate_stamp,\n",
    "                     accountcode, mail_id, ip_id, card_id, label, creationdate])\n",
    "    data = sorted(data, key = lambda k: k[-1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the raw data so one can apply ML to it \n",
    "def pre_process_data(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for item in data:\n",
    "        feats = item[0:-2]\n",
    "        label = item[-2]\n",
    "        amount_GBP = conv_curr_2_GBP (item[4], item[3])\n",
    "        feats.append(amount_GBP)\n",
    "        x.append(feats)\n",
    "        y.append(label)\n",
    "        \n",
    "    x = encode_categorical_features(x)\n",
    "    \n",
    "    return (preprocessing.scale(np.array(x), axis=0),np.array(y))\n",
    "\n",
    "#Encode the categorical features by mapping strings to integers \n",
    "def encode_categorical_features(x):\n",
    "    for item in list(issuercountry_set):\n",
    "        issuercountry_dict[item] = list(issuercountry_set).index(item)\n",
    "    for item in list(txvariantcode_set):\n",
    "        txvariantcode_dict[item] = list(txvariantcode_set).index(item)\n",
    "    for item in list(currencycode_set):\n",
    "        currencycode_dict[item] = list(currencycode_set).index(item)\n",
    "    for item in list(shoppercountry_set):\n",
    "        shoppercountry_dict[item] = list(shoppercountry_set).index(item)\n",
    "    for item in list(interaction_set):\n",
    "        interaction_dict[item] = list(interaction_set).index(item)\n",
    "    for item in list(verification_set):\n",
    "        verification_dict[item] = list(verification_set).index(item)\n",
    "    for item in list(accountcode_set):\n",
    "        accountcode_dict[item] = list(accountcode_set).index(item)\n",
    "    for item in x:\n",
    "        item[0] = issuercountry_dict[item[0]]\n",
    "        item[1] = txvariantcode_dict[item[1]]\n",
    "        item[4] = currencycode_dict[item[4]]\n",
    "        item[5] = shoppercountry_dict[item[5]]\n",
    "        item[6] = interaction_dict[item[6]]\n",
    "        item[7] = verification_dict[item[7]]\n",
    "        item[10] = accountcode_dict[item[10]]\n",
    "    return x\n",
    "\n",
    "#Convert currency to British Pounds\n",
    "def conv_curr_2_GBP (currencycode, amount):\n",
    "    rates = {'NZD':0.46, 'AUD':0.49, 'GBP':1, 'MXN':0.04, 'SEK':0.08}\n",
    "    rate = rates[currencycode]\n",
    "    return rate*amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc curve\n",
    "def plot_roc_curve(clf, y_test, x_test, name='Receiver operating characteristic'):\n",
    "    \n",
    "    # predict probabilities\n",
    "    probs = clf.predict_proba(x_test)  \n",
    "    # keep probabilities for the positive outcome only\n",
    "    probs = probs[:, 1]\n",
    "    # calculate AUC\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ML algorithm using cross-validation\n",
    "def evaluate_classifier(x,y,clf, use_PCA = False):\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "    kf = KFold(n_splits=10)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        if use_PCA:\n",
    "            pca = PCA(n_components=2)\n",
    "            pca.fit(x_train)\n",
    "            x_train = pca.transform(x_train)\n",
    "            x_test = pca.transform(x_test)\n",
    "            \n",
    "        x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "        \n",
    "        clf.fit(x_train, y_train)\n",
    "        y_predict = clf.predict(x_test)\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i]==1 and y_predict[i]==1:\n",
    "                TP += 1\n",
    "            if y_test[i]==0 and y_predict[i]==1:\n",
    "                FP += 1\n",
    "            if y_test[i]==1 and y_predict[i]==0:\n",
    "                FN += 1\n",
    "            if y_test[i]==0 and y_predict[i]==0:\n",
    "                TN += 1\n",
    "    tp_avg = TP/10\n",
    "    fp_avg = FP/10\n",
    "    fn_avg = FN/10\n",
    "    tn_avg = TN/10\n",
    "    \n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    recall = TP/(TP+FN)\n",
    "    specif = TN / (FP + TN)\n",
    "    prec = TP/(TP+FP) \n",
    "    tp_rate = TP/(TP+FN)\n",
    "    fp_rate = FP/(FP+TN)\n",
    "    \n",
    "    print ('TP: '+ str(TP))\n",
    "    print ('FP: '+ str(FP))\n",
    "    print ('FN: '+ str(FN))\n",
    "    print ('TN: '+ str(TN))\n",
    "    \n",
    "    print ('TP Rate : '+ str(tp_rate))\n",
    "    print ('FP Rate : '+ str(fp_rate))\n",
    "\n",
    "    \n",
    "    print('Accuracy:' + str(acc))\n",
    "    print('Recall:' + str(recall))\n",
    "    print('Specificity:' + str(specif))\n",
    "    print('Precision:' + str(prec))\n",
    "\n",
    "#Run the ML algorithm withour cross validation \n",
    "    TP, FP, FN, TN = 0.0000001, 0.0000001, 0.0000001, 0.0000001\n",
    "def test_classifier(x,y,clf, use_PCA=False, name='Receiver operating characteristic'):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)   \n",
    "    if ub_method != None:\n",
    "        clf = make_pipeline(ub_method, clf)\n",
    "        #x_train, y_train = ub_method.fit_resample(x_train, y_train)\n",
    "   # print(x_train)\n",
    "    if use_PCA:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(x_train)\n",
    "\n",
    "        x_train = pca.transform(x_train)\n",
    "        x_test = pca.transform(x_test)\n",
    "        #plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap=plt.cm.Set1,edgecolor='k')\n",
    "        \n",
    "    clf.fit(x_train, y_train)\n",
    "    y_predict = clf.predict(x_test)\n",
    "    for i in range(len(y_predict)):\n",
    "        if y_test[i]==1 and y_predict[i]==1:\n",
    "            TP += 1\n",
    "        if y_test[i]==0 and y_predict[i]==1:\n",
    "            FP += 1\n",
    "        if y_test[i]==1 and y_predict[i]==0:\n",
    "            FN += 1\n",
    "        if y_test[i]==0 and y_predict[i]==0:\n",
    "            TN += 1\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    recall = TP/(TP+FN)\n",
    "    specif = TN / (FP + TN)\n",
    "    prec = TP/(TP+FP) \n",
    "    tp_rate = TP/(TP+FN)\n",
    "    fp_rate = FP/(FP+TN)\n",
    "    \n",
    "    plot_roc_curve(clf, y_test, x_test, name)\n",
    "    \n",
    "    print ('TP: '+ str(TP))\n",
    "    print ('FP: '+ str(FP))\n",
    "    print ('FN: '+ str(FN))\n",
    "    print ('TN: '+ str(TN))\n",
    "    \n",
    "    print ('TP Rate : '+ str(tp_rate))\n",
    "    print ('FP Rate : '+ str(fp_rate))\n",
    "\n",
    "    \n",
    "    print('Accuracy:' + str(acc))\n",
    "    print('Recall:' + str(recall))\n",
    "    print('Specificity:' + str(specif))\n",
    "    print('Precision:' + str(prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n",
      "TP: 38\n",
      "FP: 376\n",
      "FN: 307\n",
      "TN: 235977\n",
      "TP Rate : 0.11014492753623188\n",
      "FP Rate : 0.001590840818606068\n",
      "Accuracy:0.9971144665354165\n",
      "Recall:0.11014492753623188\n",
      "Specificity:0.998409159181394\n",
      "Precision:0.09178743961352658\n",
      "Bagged SVM\n",
      "TP: 309\n",
      "FP: 37871\n",
      "FN: 36\n",
      "TN: 198482\n",
      "TP Rate : 0.8956521739130435\n",
      "FP Rate : 0.1602306719186979\n",
      "Accuracy:0.8398507803192253\n",
      "Recall:0.8956521739130435\n",
      "Specificity:0.8397693280813021\n",
      "Precision:0.008093242535358827\n",
      "Extra Random Forest Classifier with entropy criterion\n",
      "TP: 38\n",
      "FP: 289\n",
      "FN: 307\n",
      "TN: 236064\n",
      "TP Rate : 0.11014492753623188\n",
      "FP Rate : 0.0012227473313222172\n",
      "Accuracy:0.997482023506747\n",
      "Recall:0.11014492753623188\n",
      "Specificity:0.9987772526686778\n",
      "Precision:0.1162079510703364\n",
      "AdaBoost Classifier\n",
      "TP: 267\n",
      "FP: 28699\n",
      "FN: 78\n",
      "TN: 207654\n",
      "TP Rate : 0.7739130434782608\n",
      "FP Rate : 0.12142431024780731\n",
      "Accuracy:0.8784231383450641\n",
      "Recall:0.7739130434782608\n",
      "Specificity:0.8785756897521927\n",
      "Precision:0.009217703514465235\n",
      "Distance Weighed KNN\n",
      "TP: 83\n",
      "FP: 2850\n",
      "FN: 262\n",
      "TN: 233503\n",
      "TP Rate : 0.24057971014492754\n",
      "FP Rate : 0.01205823492826408\n",
      "Accuracy:0.9868524448875783\n",
      "Recall:0.24057971014492754\n",
      "Specificity:0.9879417650717359\n",
      "Precision:0.028298670303443574\n"
     ]
    }
   ],
   "source": [
    "#Pipeline\n",
    "data = get_raw_data()\n",
    "(x,y) = pre_process_data(data)\n",
    "\n",
    "fraud = 0\n",
    "non_fraud = 0\n",
    "for cls in y:\n",
    "    if cls == 0:\n",
    "        non_fraud += 1\n",
    "    if cls == 1:\n",
    "        fraud += 1\n",
    "        \n",
    "#Iterate over all classifiers\n",
    "classifiers = []\n",
    "classifiers.append((RandomForestClassifier(n_estimators=1000), \"Random Forest Classifier\"))\n",
    "classifiers.append((BaggingClassifier(base_estimator= svm.SVC(kernel = 'rbf', gamma='auto'), max_samples=0.001, bootstrap=False, n_estimators=1000, verbose=0), \"Bagged SVM\"))\n",
    "classifiers.append((ExtraTreesClassifier(n_estimators=1000, max_depth=None,min_samples_split=2, random_state=0, criterion='entropy'), \"Extra Random Forest Classifier with entropy criterion\"))\n",
    "classifiers.append((AdaBoostClassifier(n_estimators=1000), \"AdaBoost Classifier\"))\n",
    "classifiers.append((neighbors.KNeighborsClassifier(n_neighbors=5, weights = 'distance'), \"Distance Weighed KNN\"))\n",
    "\n",
    "for (clf, name) in classifiers:\n",
    "    print(name)\n",
    "    evaluate_classifier(x,y,clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasco\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "236353\n",
      "SMOTE\n",
      "TP: 73.0000001\n",
      "FP: 8106.0000001\n",
      "FN: 9.000000100000001\n",
      "TN: 39152.0000001\n",
      "TP Rate : 0.89024390148721\n",
      "FP Rate : 0.1715265140307608\n",
      "Accuracy:0.8285804816195304\n",
      "Recall:0.89024390148721\n",
      "Specificity:0.8284734859692392\n",
      "Precision:0.008925296503021755\n",
      "Near Miss\n",
      "TP: 2.0000001000000003\n",
      "FP: 5.0000001\n",
      "FN: 66.0000001\n",
      "TN: 47267.0000001\n",
      "TP Rate : 0.029411766089965402\n",
      "FP Rate : 0.00010577086012816987\n",
      "Accuracy:0.9985002112336417\n",
      "Recall:0.029411766089965402\n",
      "Specificity:0.9998942291398718\n",
      "Precision:0.2857142918367345\n"
     ]
    }
   ],
   "source": [
    "data = get_raw_data()\n",
    "(x,y) = pre_process_data(data)\n",
    "\n",
    "fraud = 0\n",
    "non_fraud = 0\n",
    "for cls in y:\n",
    "    if cls == 0:\n",
    "        non_fraud += 1\n",
    "    if cls == 1:\n",
    "        fraud += 1\n",
    "print(fraud)\n",
    "print(non_fraud)\n",
    "\n",
    "#clf = svm.LinearSVC( class_weight={0:math.sqrt(fraud), 1:math.sqrt(non_fraud)})\n",
    "clf = BaggingClassifier(base_estimator= svm.SVC(kernel = 'rbf', gamma='auto'), max_samples=0.01, bootstrap=False, n_estimators=100, verbose=0)\n",
    "\n",
    "print(\"SMOTE\")\n",
    "test_classifier(x,y,clf,ub_method = sm, use_PCA=True)\n",
    "print(\"Near Miss\")\n",
    "test_classifier(x,y,clf,ub_method = nm, use_PCA=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier(n_neighbors=5, weights = 'distance')\n",
    "print(\"SMOTE\")\n",
    "test_classifier(x,y,clf,ub_method = sm, use_PCA=False)\n",
    "print(\"Near Miss\")\n",
    "test_classifier(x,y,clf,ub_method = nm, use_PCA=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 47\n",
      "FP: 3353\n",
      "FN: 20\n",
      "TN: 43920\n",
      "TP Rate : 0.7014925373134329\n",
      "FP Rate : 0.07092843695132528\n",
      "Accuracy:0.9287494719053654\n",
      "Recall:0.7014925373134329\n",
      "Specificity:0.9290715630486747\n",
      "Precision:0.013823529411764707\n"
     ]
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=20, max_depth=None,min_samples_split=2, random_state=0, criterion='entropy')\n",
    "test_classifier(x,y,clf,use_PCA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 50.0000001\n",
      "FP: 2551.0000001\n",
      "FN: 28.0000001\n",
      "TN: 44711.0000001\n",
      "TP Rate : 0.6410256406640368\n",
      "FP Rate : 0.05397570987451239\n",
      "Accuracy:0.9455217574951793\n",
      "Recall:0.6410256406640368\n",
      "Specificity:0.9460242901254876\n",
      "Precision:0.01922337566172831\n",
      "TP: 71.0000001\n",
      "FP: 6424.0000001\n",
      "FN: 10.000000100000001\n",
      "TN: 40835.0000001\n",
      "TP Rate : 0.8765432089468069\n",
      "FP Rate : 0.13593178019155744\n",
      "Accuracy:0.8640895648469448\n",
      "Recall:0.8765432089468069\n",
      "Specificity:0.8640682198084425\n",
      "Precision:0.010931485773335442\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1000)\n",
    "test_classifier(x,y,clf,ub_method = sm, use_PCA=False)\n",
    "test_classifier(x,y,clf,ub_method = sm, use_PCA=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.51126894e-08 6.92688185e-09 3.84184955e-04 1.33245499e-04\n",
      " 2.77075274e-09 7.96591412e-08 0.00000000e+00 6.92688185e-10\n",
      " 0.00000000e+00 9.99999897e-01 1.38537637e-09 1.06998851e-04\n",
      " 7.70698728e-05 1.51829631e-04 5.32981997e-06]\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
