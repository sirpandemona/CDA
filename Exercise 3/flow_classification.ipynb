{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaddata import *\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('datasets/capture20110811.pcap.netflow.labeled')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\n",
    "df['Flags']= df['Flags'].astype(\"category\")\n",
    "df['Prot']= df['Prot'].astype(\"category\")\n",
    "\n",
    "flag_cat = df['Flags'].astype(\"category\").cat\n",
    "prot_cat = df['Prot'].astype(\"category\").cat\n",
    "\n",
    "df['Flags']= df['Flags'].cat.codes\n",
    "df['Prot']= df['Prot'].cat.codes\n",
    "\n",
    "\n",
    "features = df[['Durat', 'Prot', 'Flags', 'Tos', 'Packets','Bytes','Flows']]\n",
    "labels = df[['Label']]\n",
    "hosts = df[['src_ip', 'src_port', 'dst_ip','dst_port']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to np_arrays \n",
    "x= features.values\n",
    "y = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x).to_csv(\"flow_class_packet_x.csv\", header=None, index=None)\n",
    "pd.DataFrame(y).to_csv(\"flow_class_packet_y.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data aggeregation \n",
    "src_ips = df['src_ip'].unique()\n",
    "aggr_x = []\n",
    "aggr_y = []\n",
    "\n",
    "grouped = df.groupby(by='src_ip')\n",
    "for src_ip, relevant_flows in grouped:\n",
    "    src_ports = len(relevant_flows['src_port'].unique())\n",
    "    dst_ports = len(relevant_flows['dst_port'].unique())\n",
    "    dst_addr = len( relevant_flows['dst_ip'].unique())\n",
    "    netflows = relevant_flows['Flows'].sum()\n",
    "    byts = relevant_flows['Bytes'].sum()\n",
    "    packets = relevant_flows['Packets'].sum()\n",
    "    labels = relevant_flows[relevant_flows['Label'] == 'Botnet']\n",
    "    feats = [src_ports, dst_ports,dst_addr, netflows, byts, packets]\n",
    "    aggr_x.append(feats)\n",
    "    #generate class\n",
    "    lab = 0\n",
    "\n",
    "    if(len(labels) > 0):\n",
    "        lab = 1\n",
    "    aggr_y.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_x = np.array(aggr_x)\n",
    "aggr_y = np.array(aggr_y)\n",
    "pd.DataFrame(aggr_x).to_csv(\"flow_class_packet_x_aggr.csv\", header=None, index=None)\n",
    "pd.DataFrame(aggr_y).to_csv(\"flow_class_packet_y_aggr.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"flow_class_packet_x.csv\").values\n",
    "y = pd.read_csv(\"flow_class_packet_y.csv\").values\n",
    "aggr_x = pd.read_csv(\"flow_class_packet_x_aggr.csv\").values\n",
    "aggr_y = pd.read_csv(\"flow_class_packet_y_aggr.csv\").values\n",
    "def clean_y(y):\n",
    "    isBotnet = y == 'Botnet'\n",
    "    y[isBotnet] = 1\n",
    "    y[~isBotnet] = 0\n",
    "    return y\n",
    "y = clean_y(y)\n",
    "y=np.ravel(y).astype(int)\n",
    "aggr_y=np.ravel(aggr_y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6296754,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_idx[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr_y1[aggr_y1 ==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(x,y,a):\n",
    "    norm_idx = np.where(y == 0)[0]\n",
    "    botnet_idx = np.where(y == 1)[0]\n",
    "    n = len(norm_idx)\n",
    "    sample_size = math.ceil(n*a)\n",
    "    smpl_idx = np.random.choice(norm_idx,sample_size)\n",
    "    smpl_idx= np.append(smpl_idx,botnet_idx)\n",
    "    return (x[smpl_idx,:],y[smpl_idx])\n",
    "    \n",
    "(x1, y1) = undersample(x,y,0.01)    \n",
    "(aggr_x1, aggr_y1) = undersample(aggr_x,aggr_y,0.01)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ML algorithm using cross-validation\n",
    "def evaluate_classifier(x,y,clf, verbose=False):\n",
    "    TP, FP, FN, TN = 0.0001, 0.0001, 0.0001, 0.0001\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        #Initialize training and test data\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        #Resamples the training data using SMOTE\n",
    "        (x_train, y_train) = undersample(x_train, y_train,0.01)  \n",
    "        #Fits classifier to training data\n",
    "        clf.fit(x_train, y_train)\n",
    "        #Predict data for testing data\n",
    "        y_predict = clf.predict(x_test)\n",
    "        #Enumerate results\n",
    "        print(\"*\", end = '')\n",
    "        for i in range(len(y_predict)):\n",
    "            if y_test[i]==1 and y_predict[i]==1:\n",
    "                TP += 1\n",
    "            if y_test[i]==0 and y_predict[i]==1:\n",
    "                FP += 1\n",
    "            if y_test[i]==1 and y_predict[i]==0:\n",
    "                FN += 1\n",
    "            if y_test[i]==0 and y_predict[i]==0:\n",
    "                TN += 1\n",
    "    print(\"\")\n",
    "    tp_avg = TP/10\n",
    "    fp_avg = FP/10\n",
    "    fn_avg = FN/10\n",
    "    tn_avg = TN/10\n",
    "    \n",
    "    #Calculates the metrics\n",
    "    acc = (TP+TN)/(TP+FP+TN+FN) #Accuracy\n",
    "    recall = TP/(TP+FN) #Recall\n",
    "    specif = TN / (FP + TN) #Specificity\n",
    "    prec = TP/(TP+FP) #Precision\n",
    "    fp_rate = FP/(FP+TN) #False Positive Rate\n",
    "    \n",
    "    #Print metrics\n",
    "    if(verbose):\n",
    "        print ('TP: '+ str(TP))\n",
    "        print ('FP: '+ str(FP))\n",
    "        print ('FN: '+ str(FN))\n",
    "        print ('TN: '+ str(TN))\n",
    "        print ('FP Rate : '+ str(fp_rate))\n",
    "        print('Accuracy:' + str(acc))\n",
    "        print('Recall:' + str(recall))\n",
    "        print('Specificity:' + str(specif))\n",
    "        print('Precision:' + str(prec))\n",
    "        \n",
    "    return [tp_avg, fp_avg, fn_avg, tn_avg, acc, recall, specif, prec, fp_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Weighed KNN\n",
      "Host lvl\n",
      "**********\n",
      "Package lvl\n",
      "**********\n",
      "AdaBoost Classifier\n",
      "Host lvl\n",
      "**********\n",
      "Package lvl\n",
      "**********\n",
      "Random Forest Classifier\n",
      "Host lvl\n",
      "**********\n",
      "Package lvl\n",
      "**********\n",
      "Bagged SVM\n",
      "Host lvl\n",
      "**********\n",
      "Package lvl\n",
      "**********\n",
      "Extra Random Forest Classifier with entropy criterion\n",
      "Host lvl\n",
      "**********\n",
      "Package lvl\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "#Create some classifiers\n",
    "classifiers = []\n",
    "classifiers.append((neighbors.KNeighborsClassifier(n_neighbors=3, weights = 'distance'), \"Distance Weighed KNN\"))\n",
    "classifiers.append((AdaBoostClassifier(n_estimators=200), \"AdaBoost Classifier\"))\n",
    "classifiers.append((RandomForestClassifier(n_estimators=200), \"Random Forest Classifier\"))\n",
    "classifiers.append((BaggingClassifier(base_estimator= svm.SVC(kernel = 'rbf', gamma='auto'), max_samples=0.001, bootstrap=False, n_estimators=1000, verbose=0), \"Bagged SVM\"))\n",
    "classifiers.append((ExtraTreesClassifier(n_estimators=200, max_depth=None,min_samples_split=2, random_state=0, criterion='entropy'), \"Extra Random Forest Classifier with entropy criterion\"))\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "res_pck = []\n",
    "res_hst = []\n",
    "#Iterate over all classifiers\n",
    "for (clf, name) in classifiers:\n",
    "    print(name)\n",
    "    print(\"Host lvl\")\n",
    "    res_hst.append(evaluate_classifier(aggr_x,aggr_y,clf))\n",
    "    print(\"Package lvl\")\n",
    "    res_pck.append(evaluate_classifier(x,y,clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res_pck).to_csv(\"results_pack_lvl.csv\", header=None, index=None)\n",
    "pd.DataFrame(res_hst).to_csv(\"results_host_lvl.csv\", header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4258, 6)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggr_x1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
